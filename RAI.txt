Responsible AI is a governance framework that documents how a specific organization is addressing the challenges around artificial intelligence (AI) from both an ethical and legal point of view. Resolving ambiguity for where responsibility lies if something goes wrong is an important driver for responsible AI initiatives.

As of this writing, the development of fair, trustworthy AI standards is up to the discretion of the data scientists and software developers who write and deploy a specific organization's AI algorithmic models. This means that the steps required to prevent discrimination and ensure transparency vary from company to company.

Just as ITIL provided a common framework for delivering IT services, proponents of responsible AI hope that a widely adopted governance framework of AI best practices will make it easier for organizations around the globe to ensure their AI programming is human-centered, interpretable and explainable.

In a large enterprise, the chief analytics officer (CAO) is typically tasked with developing, implementing and monitoring the organization's responsible AI framework. Typically documented on the organization's website, the framework explains in simple language how the organization addresses accountability and ensures their use of AI is anti-discriminatory.

Why responsible AI is important
Responsible AI is an emerging area of AI governance and use of the word "responsible" is an umbrella term that covers both ethics and democratization.

The heads of Microsoft and Google have publicly called for AI regulations, but as of this writing, there are no standards for accountability when AI programming creates unintended consequences. Often, bias can be introduced into AI by the data that's used to train machine learning models. When the training data is biased, it naturally follows that decisions made by the programming are also biased.

Now that software programs with artificial intelligence (AI) features are becoming more common, it is increasingly apparent that there is a need for standards in AI beyond those established by Isaac Asimov in his "Three Laws of Robotics." The technology can be misused accidentally (or on purpose) for a number of reasons -- and much of the misuse is caused by a bias in the selection of data to train AI programming.

What are the principles of responsible AI?
AI and the machine learning models that support it should be comprehensive, explainable, ethical and efficient.

Comprehensiveness â€“ comprehensive AI has clearly defined testing and governance criteria to prevent machine learning from being hacked easily.
explainable AI is programmed to describe its purpose, rationale and decision-making process in a way that can be understood by the average end user.
Ethical AI initiatives have processes in place to seek out and eliminate bias in machine learning models.
Efficient AI is able to run continually and respond quickly to changes in the operational environment.
Why is responsible AI important?
An important goal of responsible AI is to reduce the risk that a minor change in an input's weight will drastically change the output of a machine learning model.

Within the context of conforming to the four tenets of corporate governance, responsible AI should be:

Each step of the model development process should be recorded in a way that cannot be altered by humans or other programming.
The data used to train machine models should not be biased.
The analytic models that support an AI initiative can be adapted to changing environments without introducing bias.
The organization deploying AI programming is sensitive to its potential impact -- both positive and negative.
How do you design responsible AI?
Building a responsible AI governance framework can be a lot of work. Ongoing scrutiny is crucial to ensure an organization is committed to providing an unbiased, trustworthy AI. This is why it is crucial for an organization to have a maturity model or rubric to follow while designing and implementing an AI system.

At a base level, to be considered responsible, AI must be built with resources and technology according to a company-wide development standard that mandates the use of:

Shared code repositories
Approved model architectures
Sanctioned variables
Established bias testing methodologies to help determine the validity of tests for AI systems
Stability standards for active machine learning models to make sure AI programming works as intended
